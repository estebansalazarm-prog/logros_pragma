# Job Glue: col_pragma_logro_pgm_extraer_tabla_dynamodb

## ğŸ“‹ Tabla de Contenidos
- [DescripciÃ³n General](#descripciÃ³n-general)
- [Arquitectura del Job](#arquitectura-del-job)
- [ConfiguraciÃ³n YAML](#configuraciÃ³n-yaml)
- [Estructura del CÃ³digo](#estructura-del-cÃ³digo)
- [Flujo de EjecuciÃ³n](#flujo-de-ejecuciÃ³n)
- [Agregar Nueva Tabla](#agregar-nueva-tabla)
- [Troubleshooting](#troubleshooting)

## ğŸ“– DescripciÃ³n General

### Â¿QuÃ© hace este job?
Job **dinÃ¡mico y configurable** que ingesta tablas de DynamoDB exportadas a S3 sin necesidad de modificar cÃ³digo. Toda la configuraciÃ³n se maneja mediante archivos YAML.

### Responsabilidades principales:
1. âœ… Lee exportaciones de DynamoDB desde S3 (formato JSON)
2. âœ… Aplana estructura DynamoDB (Item/NewImage â†’ columnas planas)
3. âœ… Aplica transformaciones configuradas en YAML
4. âœ… Guarda en Analytics (Parquet) y Curated (Hudi)

### Modos de ejecuciÃ³n:
- **FULL**: ExportaciÃ³n completa de la tabla
- **INC**: ExportaciÃ³n incremental (CDC - Change Data Capture)

### Control de Versiones
| VersiÃ³n | DescripciÃ³n | Autor | Fecha |
|---------|-------------|-------|-------|
| 1.0 | CreaciÃ³n del job dinÃ¡mico | Esteban Salazar y Oscar Vergara| 2025-01-30 |

## ğŸ—ï¸ Arquitectura del Job

### Componentes principales:
```
col_pragma_logro_pgm_extraer_tabla_dynamodb
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ reports/              # ğŸ“„ Archivos YAML de configuraciÃ³n
â”‚   â”‚   â”œâ”€â”€ nombre_tabla.yml
â”‚   â”œâ”€â”€ report_config.py      # Carga y parsea YAML
â”‚   â””â”€â”€ sources_dictionary.py # VacÃ­o (legacy, no se usa)
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ extract/
â”‚   â”‚   â”œâ”€â”€ get_sources_options.py  # Lee path S3 desde YAML
â”‚   â”‚   â””â”€â”€ sources.py              # Carga DataFrames dinÃ¡micamente
â”‚   â”œâ”€â”€ transform/
â”‚   â”‚   â”œâ”€â”€ raw_transformations.py  # LÃ³gica de transformaciÃ³n
â”‚   â”‚   â””â”€â”€ transformations.py      # Orquestador
â”‚   â””â”€â”€ load/
â”‚       â”œâ”€â”€ metadata.py             # Agrega columnas de metadata
â”‚       â””â”€â”€ save.py                 # Guarda en Analytics/Curated
â””â”€â”€ col_pragma_logro_pgm_extraer_tabla_dynamodb.py  # ğŸš€ Main
```

### Flujo de datos:
```
S3 Raw (JSON) â†’ Flatten DynamoDB â†’ Transformaciones â†’ Analytics (Parquet) â†’ Curated (Hudi)
```

## âš™ï¸ ConfiguraciÃ³n YAML

### UbicaciÃ³n:
```
col_pragma_logro_pgm_extraer_tabla_dynamodb/config/reports/<nombre_tabla>.yml
```

### Estructura completa:
```yaml
process_metadata:
  process_id: 'extract_<tabla>'
  description: 'DescripciÃ³n del proceso'
  owner: 'pragma'
  version: '1.0.0'

source_config:
  source_name: 'Nombre descriptivo de la fuente'
  connection_type: 's3_path'
  s3_details:
    path_template_inc: "s3://{raw_bucket}/path/incremental/..."
    path_template_full: "s3://{raw_bucket}/path/full/..."
    data_format: "json"

processing_config:
  schema_name: "col_<product_type>_<tabla>"
  key_columns:
    - "key"
    - "sortkey"
  order_by_column: "tstamp"
  partition_date: "created_at"
  precombine_key: "tstamp"

output_config:
  data_product: "<nombre_tabla>"
  product_type: "<tipo_producto>"
```

### Campos clave del YAML:

| SecciÃ³n | Campo | DescripciÃ³n | Ejemplo |
|---------|-------|-------------|---------|
| **source_config** | `path_template_inc` | Path S3 para incremental | `s3://.../incremental/...` |
| | `path_template_full` | Path S3 para full | `s3://.../full/...` |
| **processing_config** | `schema_name` | Nombre del schema (.schema.yml) | `col_pragma_tabla_final` |
| | `key_columns` | Columnas para filtro NOT NULL | `["key", "sortkey"]` |
| | `order_by_column` | Columna para window function | `"tstamp"` |
| | `partition_date` | Columna para particionar | `"created_at"` |
| | `precombine_key` | Columna para Hudi precombine | `"tstamp"` |
| **output_config** | `data_product` | Nombre de la tabla final | `"tabla_final"` |
| | `product_type` | Tipo de producto | `"pragma"` |

### Tablas configuradas actualmente:

| Tabla | YAML | Product Type | Proveedor |
|-------|------|--------------|-----------|
| tabla_final | nombre_tabla.yml | pragma | Pragma |


## ğŸ”§ Estructura del CÃ³digo

### 1. Main (col_pragma_logro_pgm_extraer_tabla_dynamodb.py)
```python
# 1. Carga configuraciÃ³n YAML
elt_config = load_etl_config_from_yaml(table_name=CONFIG_TABLE, job_config=job_config)

# 2. Crea catÃ¡logo con funciÃ³n personalizada
catalog = Catalog(glue_context, job_config, get_table_func=get_sources_options_report)

# 3. Carga sources dinÃ¡micamente
sources = RawSources(catalog, pre_load_sources)

# 4. Ejecuta transformaciones
transformations = Transformations(spark, job_config, sources)
df = transformations.get_main_table()

# 5. Agrega metadata y guarda
df_final = add_metadata(df, job_config)
save_analytics(df_final, job_config)
save_action(df_analytics, job_config.hudi_options, job_config.curated_table_path)
```

### 2. Carga de configuraciÃ³n (config/report_config.py)
```python
def load_etl_config_from_yaml(table_name, job_config):
    # Lee YAML y extrae data_product, precombine_key, product_type
    # Actualiza job_config.constants con estos valores
    # Reemplaza placeholders: {raw_bucket}, {process_year}, etc.
    # Retorna ETLTableConfig con toda la configuraciÃ³n
```

### 3. ExtracciÃ³n (etl/extract/get_sources_options.py)
```python
def get_sources_options_report(table_name, job_config):
    # Lee source_config del YAML
    # Selecciona path_template_inc o path_template_full segÃºn PROCESS_TYPE
    # Retorna S3File(bucket, prefix, origin, format)
```

### 4. Transformaciones (etl/transform/raw_transformations.py)
```python
def get_table_transformations(df_dinamic, job_config):
    # 1. Aplana estructura DynamoDB (Item o NewImage)
    # 2. Filtra NULL en key_columns
    # 3. Window function para Ãºltimo estado
    # 4. Genera columna 'id' (concatenaciÃ³n de key_columns)
    # 5. Alinea schema con .schema.yml
    # 6. Convierte timestamps
```

### 5. Carga (etl/load/save.py)
```python
# Analytics: Parquet, mode=overwrite
save_analytics(df, job_config)

# Curated: Hudi, mode=upsert
save_action(df, hudi_options, path)
```

## ğŸš€ Flujo de EjecuciÃ³n

### ParÃ¡metros del Job

| ParÃ¡metro | DescripciÃ³n | Valores | Ejemplo |
|-----------|-------------|---------|---------|
| ACCOUNT | Cuenta AWS | String | "123456789" |
| ENV | Ambiente de ejecuciÃ³n | dev, qa, pdn | "dev" |
| PROCESS_DATE | Fecha de proceso | YYYY-MM-DD | "2025-01-30" |
| PROCESS_TYPE | Tipo de proceso | FULL, INC | "INC" |
| CONFIG_TABLE | Nombre del archivo YAML (sin extensiÃ³n) | String | "nombre_tabla" |

### Paso a paso:

#### 1ï¸âƒ£ Carga de ConfiguraciÃ³n
```python
elt_config = load_etl_config_from_yaml(CONFIG_TABLE, job_config)
# - Lee config/reports/{CONFIG_TABLE}.yml
# - Extrae: data_product, precombine_key, product_type
# - Actualiza job_config.constants
# - Reemplaza placeholders: {raw_bucket}, {process_year}, {process_month}, etc.
```

#### 2ï¸âƒ£ ExtracciÃ³n de Datos
```python
catalog = Catalog(glue_context, job_config, get_sources_options_report)
sources = RawSources(catalog, pre_load_sources)
# - get_sources_options_report() lee path S3 desde YAML
# - Selecciona path_template_inc o path_template_full
# - Carga DataFrame desde S3 (formato JSON)
```

#### 3ï¸âƒ£ Transformaciones
```python
df = transformations.get_main_table()
# - flatten_dynamodb_struct(): Aplana Item/NewImage
# - Filtra NULL en key_columns (del YAML)
# - Window function: partition_by(key_columns).orderBy(order_by_column)
# - Genera 'id': concat_ws("-", key_columns)
# - align_schema(): Alinea con schema_name.schema.yml
# - Convierte timestamps (partition_date)
```

#### 4ï¸âƒ£ Metadata y Carga
```python
df_final = add_metadata(df, job_config)
# - Agrega: momento_ingestion, job_process_date, year, month, day

save_analytics(df_final, job_config)
# - Formato: Parquet
# - Modo: overwrite
# - Path: s3://.../analytics/.../co_{product_type}_{data_product}/

save_action(df_analytics, hudi_options, curated_table_path)
# - Formato: Hudi
# - Modo: upsert
# - Primary key: id
# - Precombine key: del YAML
# - Path: s3://.../curated/.../co_{product_type}_{data_product}/
```

### Diagrama visual:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Cargar YAML (CONFIG_TABLE)                           â”‚
â”‚     â””â”€> Actualiza job_config.constants                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Leer S3 (path_template_inc/full)                     â”‚
â”‚     â””â”€> DataFrame JSON (DynamoDB export)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. Transformaciones                                      â”‚
â”‚     â”œâ”€> Flatten (Item/NewImage)                          â”‚
â”‚     â”œâ”€> Filter (key_columns NOT NULL)                    â”‚
â”‚     â”œâ”€> Window (Ãºltimo estado)                           â”‚
â”‚     â”œâ”€> Generar 'id'                                     â”‚
â”‚     â”œâ”€> Align schema                                     â”‚
â”‚     â””â”€> Convert timestamps                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. Metadata + Particiones                               â”‚
â”‚     â””â”€> year, month, day, momento_ingestion              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. Save Analytics (Parquet, overwrite)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. Save Curated (Hudi, upsert)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Rutas de datos:

| Zona | Path | Formato | Modo |
|------|------|---------|------|
| **Raw** | `s3://{BUCKET}/{PREFIX}/{process_type}/year={year}/month={month}/day={day}/[hour={hour}/]AWSDynamoDB/[*/]data` | JSON | Read |
| **Curated** | `s3://{BUCKET}/{PREFIX}/` | Hudi | Upsert |

## â• Agregar Nueva Tabla

### Checklist completo:

#### âœ… Paso 1: Crear YAML de configuraciÃ³n
```bash
cd col_pragma_logro_pgm_extraer_tabla_dynamodb/config/reports/
cp nombre_tabla.yml nueva_tabla.yml
```

**Editar nueva_tabla.yml:**
```yaml
process_metadata:
  process_id: 'extract_nueva_tabla'
  description: 'DescripciÃ³n de la nueva tabla'
  owner: 'pragma'

source_config:
  source_name: 'Nombre descriptivo'
  s3_details:
    path_template_inc: "s3://{raw_bucket}/.../nueva_tabla/incremental/..."
    path_template_full: "s3://{raw_bucket}/.../nueva_tabla/full/..."

processing_config:
  schema_name: "co_{product_type}_nueva_tabla"  # âš ï¸ Debe coincidir con .schema.yml
  key_columns: ["key", "sortkey"]  # Columnas PK de DynamoDB
  order_by_column: "tstamp"  # Para window function
  partition_date: "created_at"  # Para particiones year/month/day
  precombine_key: "tstamp"  # Para Hudi

output_config:
  data_product: "nueva_tabla"
  product_type: "pragma" 
```

#### âœ… Paso 2: Crear schema YAML
```bash
cd addons/config/schemas/
cp col_pragma_tabla_final.schema.yml col_{product_type}_nueva_tabla.schema.yml
```

**Editar schema:**
- Definir todas las columnas con sus tipos
- Incluir columnas de metadata: `id`, `momento_ingestion`, `job_process_date`
- Formato: Lista de diccionarios con `Name`, `Type`, `Nullable`

#### âœ… Paso 3: Actualizar MANIFEST.in
```bash
cd glue/
vim MANIFEST.in
```
Agregar:
```
include addons/config/schemas/co_{product_type}_nueva_tabla.schema.yml
include col_pragma_logro_pgm_extraer_tabla_dynamodb/config/reports/nueva_tabla.yml
```

#### âœ… Paso 4: Probar localmente
```bash
spark-submit col_pragma_logro_pgm_extraer_tabla_dynamodb.py \
  --ACCOUNT=123456789 \
  --ENV=dev \
  --PROCESS_DATE=2025-01-30 \
  --PROCESS_TYPE=FULL \
  --CONFIG_TABLE=nueva_tabla
```

#### âœ… Paso 5: Crear tests
```bash
cd tests/
cp test_srf_curado_extraer_tabla_dynamodb_full.py test_nueva_tabla_full.py
```

### âš ï¸ Puntos crÃ­ticos:

| Aspecto | ValidaciÃ³n |
|---------|------------|
| **Nombre schema** | `processing_config.schema_name` debe coincidir con archivo `.schema.yml` |
| **Key columns** | Deben existir en el DataFrame despuÃ©s de flatten |
| **Order by column** | Debe ser numÃ©rico/timestamp para ordenar correctamente |
| **Partition date** | Debe ser timestamp para generar year/month/day |
| **Paths S3** | Verificar que existan exportaciones en esas rutas |

## ğŸ› Troubleshooting

### Errores Comunes

| ERROR | DESCRIPCIÃ“N | POSIBLE SOLUCIÃ“N |
|-------|-------------|------------------|
| FileNotFoundError: Archivo de configuraciÃ³n no encontrado | El archivo YAML no existe en config/reports/ | Verificar que el archivo {CONFIG_TABLE}.yml exista en la ruta correcta |
| ValueError: No se pudo cargar la configuraciÃ³n ETL desde YAML | Error al parsear el YAML | Validar sintaxis del YAML, verificar que todos los campos requeridos estÃ©n presentes |
| AnalysisException: Column 'key' cannot be resolved | Las columnas en key_columns no existen en el DataFrame | Verificar que key_columns en el YAML coincidan con las columnas del schema despuÃ©s de aplanar |
| FileNotFoundException: Path does not exist | No se encuentra el archivo en S3 | Verificar que la exportaciÃ³n de DynamoDB se haya completado correctamente en la ruta configurada |
| Error de memoria insuficiente en Glue Job | Datos muy grandes para la configuraciÃ³n actual | Aumentar DPU del job o ajustar coalesce(8) en el cÃ³digo |
| Schema mismatch | El schema del archivo no coincide con el esperado | Verificar que el archivo .schema.yml estÃ© actualizado con la estructura correcta |
| Hudi commit failed | Error al escribir en formato Hudi | Verificar permisos IAM, revisar logs de Hudi, validar que no haya conflictos de escritura concurrente |
| Timeout reading from S3 | Lectura de S3 excede el tiempo lÃ­mite | Verificar conectividad de red, aumentar timeout, revisar tamaÃ±o de archivos |
| Invalid timestamp conversion | Error al convertir columnas de timestamp | Verificar que partition_date en YAML sea una columna vÃ¡lida y tenga formato correcto |
| Duplicate key error in Hudi | Registros duplicados con la misma primary key | Revisar lÃ³gica de window function, validar que order_by_column sea correcto |

### ğŸ“Š Monitoreo

**CloudWatch Logs:**
- Output: `/aws-glue/jobs/output`
- Error: `/aws-glue/jobs/error`

**MÃ©tricas clave:**
- Registros procesados: `df_final.count()` en logs
- Tiempo de ejecuciÃ³n: Consola de Glue
- Errores: CloudWatch

**Validaciones automÃ¡ticas:**
- âœ… Datos vacÃ­os â†’ retorna sin error
- âœ… Filtros NOT NULL en key_columns
- âœ… ConversiÃ³n de tipos con manejo de errores
- âœ… AlineaciÃ³n de schema (agrega columnas faltantes como NULL)

### ğŸ” Debug tips

**Ver configuraciÃ³n cargada:**
```python
print(job_config.elt_config_table.processing_config.schema_name)
print(job_config.elt_config_table.source_config.s3_details.path_template_inc)
```

**Ver DataFrame despuÃ©s de flatten:**
```python
df_flattened.printSchema()
df_flattened.show(5, truncate=False)
```

**Ver columnas generadas:**
```python
print("Columnas despuÃ©s de align_schema:", df_aligned_schema.columns)
```
